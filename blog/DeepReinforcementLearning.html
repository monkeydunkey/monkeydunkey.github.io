<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-96x96.png">
    <title>Landing a shuttle on Moon with Deep Reinforcement Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="../ThirdParty/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../CSS/blog.css" rel="stylesheet">
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="blog-nav">
          <a class="blog-nav-item active" href="../index.html">Home</a>
          <a class="blog-nav-item" href="../resources.html">Resources</a>
          <a class="blog-nav-item" href="../Aboutme.html">About</a>
        </nav>
      </div>
    </div>

    <div class="container">

      <div class="blog-header">
        <h1 class="blog-title">Landing a shuttle on Moon with Deep Reinforcement Learning</h1>
        <p class="lead blog-description">Using Deep Q Learning to land a shuttle</p>
      </div>


            <div class="row">
              <div class="col-sm-12 blog-main">
                <div class="blog-post">
                  <p>
                    Using Reinforcement learning techniques to train bot that can play 2D video games was something
                    I wanted to try from a long time. Fortunately as a part of the Artifical Intelligence
                    course that I took this semester, I got to implement just that. The task was to train a
                    Deep Reinforcement Leanrning agent that can learn how to properly land a shuttle on the
                    surface of moon. The game evnvironemnt was Open AI's
                    <a href="https://gym.openai.com/envs/LunarLander-v2/">Lunar Landar </a>. The particular
                    technique that was used was Q learning, which is same as what Deep Mind used in its landmark
                    2013 paper on using <a href="https://arxiv.org/pdf/1312.5602v1.pdf"> Deep Reinforcement Learning to play Atari </a>.
                    This post in no way covers all the concepts requried for implementing such a system from scratch rather I will try
                    to provide a brief overview of the concepts related. Finally, in no way I am claiming that following whatever
                    I am about to say will lead to results similar to what I have as neural network are pretty hard to train correctly
                    and also they are pretty much non-deterministic in nature. So what I will try to do is point out the problems I faced
                    while training my network and the steps I took to solve them.
                  </p>
                  <p>
                    Before we begin, a quick introduction to basic of Q learning and how we can modify it to use it with a
                    neural network. The optimal Q-value for a state action pair according to the Bellman equation is given
                    by the following equation
                          $$ Q*(s, a) = \sum_{s'}p(s'|s, a)(R(s, a, s') + \gamma*\max_a Q(s', a)) $$
                    here Q(s, a) is the Q-value of a state action pair, p(s'|s, a) is the probability of reaching state s'
                    by performing action a from state s (by using probability we are taking into account the uncertainity associated
                    with taking an action),  R(s, a, s') is the reward for the action and \( \gamma \) is the discount factor which
                    represents by how much should future rewards should affect actions in current state. A high \( \gamma \) would
                    lead to the bot favoring higher rewards in the future where as lower values will make it look for more
                    immediate rewards
                  </p>
                  <p>
                    In our current scenario as we are playing a game we can assume that whatever action we take gets executed
                    with 100% certainity, thus the above equations becomes:
                     $$ Q*(s, a) = R(s, a, s') + \gamma*\max_a Q(s', a) $$
                    One question that came to my mind while training the neural network was that why to do it this way
                    i.e. why use a neural network which is very very hard to train when you can simply use methods like
                    value iteration and policy iteration which are deterministic and very easy to implement. The reason
                    for this is that though Q value update using Bellman is correct in theory (Bellman equation basically says
                    start optimal and be optimal at every step of the way) its implementation using value or policy iteration
                    uses discrete state spaces, which is something we rarely come accross in real world problems. We could
                    always try and discretize the space but this would usually lead us to store a lot of state information,
                    many of which would be about nearby states which can have similar Q values. This is were a neural network
                    comes in, it can be used to produce Q-values from a continous state space.
                  </p>
                  <p>
                    Now that we know what the Q-value equation looks like and why we should use neural network to solve it, lets
                    look at how exactly we need to do this. Anyone familiar with neural network or with any other optimization
                    strategy in general will know that we need a loss value to minimize in order for things to work. A good loss
                    value in this case can be the mean squared difference between the Q-value returned by the Bellman equation
                    and the one returned by the neural network. This makes sense as we would like to train our model such that
                    the Q-values produced are exactly same as what we would have achieved using methods like value iteration if
                    we would have discretized the state space. So our loss functions looks like the following.
                    $$ L = mean((Bellman - Q(s, a))^2) $$
                    Subsituiting the equation for Bellman value we solved above, we get
                    $$ L = mean((R(s, a, s') + \gamma*\max_a Q(s', a) - Q(s, a))^2) $$
                  </p>
                  <p>
                    Now that we have all the maths figured out, its time to get into the technical details and implement it
                    using Tensorflow
                  </p>
                </div><!-- /.blog-post -->
              </div><!-- /.blog-main -->
            </div><!-- /.row -->
            <div id="disqus_thread"></div>
            <script>
                /**
                *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
                /*
                var disqus_config = function () {
                this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
                this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                };
                */
                (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');
                s.src = 'https://https-monkeydunkey-github-io.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div><!-- /.container -->

    <footer class="blog-footer">
      <p>Blog template built for <a href="http://getbootstrap.com">Bootstrap</a> by <a href="https://twitter.com/mdo">@mdo</a>.</p>
      <p>A shout out to Rob Hust for the fantastic icon.</p>
      <p>
        <a href="#">Back to top</a>
      </p>
    </footer>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../ThirdParty/bootstrap/js/bootstrap.min.js"></script>

  </body>
</html>
