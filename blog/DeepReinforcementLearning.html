<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon-96x96.png">
    <title>Landing a shuttle on Moon with Deep Reinforcement Learning</title>

    <!-- Bootstrap core CSS -->
    <link href="../ThirdParty/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../CSS/blog.css" rel="stylesheet">
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/languages/go.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="blog-nav">
          <a class="blog-nav-item active" href="../index.html">Home</a>
          <a class="blog-nav-item" href="../resources.html">Resources</a>
          <a class="blog-nav-item" href="../Aboutme.html">About</a>
        </nav>
      </div>
    </div>

    <div class="container">

      <div class="blog-header">
        <h1 class="blog-title">Landing a shuttle on Moon with Deep Reinforcement Learning</h1>
        <p class="lead blog-description">Using Deep Q Learning to land a shuttle</p>
      </div>


            <div class="row">
              <div class="col-sm-12 blog-main">
                <div class="blog-post">
                  <h4>Introduction<h4>
                  <p>
                    Using Reinforcement learning techniques to train bot that can play 2D video games was something
                    I wanted to try from a long time. Fortunately as a part of the Artifical Intelligence
                    course that I took this semester, I got to implement just that. The task was to train a
                    Deep Reinforcement Leanrning agent that can learn how to properly land a shuttle on the
                    surface of moon. The game evnvironemnt was Open AI's
                    <a href="https://gym.openai.com/envs/LunarLander-v2/">Lunar Landar </a>. The particular
                    technique that I used was Q learning, which is same as what Deep Mind used in its landmark
                    2013 paper on using <a href="https://arxiv.org/pdf/1312.5602v1.pdf"> Deep Reinforcement Learning to play Atari breakout </a>.
                    This post in no way covers all the concepts requried for implementing such a system from scratch, rather I will try
                    to provide a brief overview of the concepts related. Finally, in no way I am claiming that following whatever
                    I am about to say will lead to results similar to what I have, as neural network are pretty hard to train correctly
                    and require lot of tweaking to work correctly due to their non-deterministic nature. So what I will try to
                    do is point out the problems I faced while training my network and the steps I took to solve them.
                  </p>
                  <figure>
                      <img style = "display: block;margin: 0 auto; width:50%" src="../resources/images/lunarLanderGame.png" alt='Lunar Lander Game' />
                      <figcaption style = "text-align:center">The aim is to land the lander between the flags as softly as possible</figcaption>
                  </figure>
                  <p>
                    Before we begin, a quick introduction to basic of Q learning and how we can use it with a
                    neural network. The optimal Q-value for a state action pair according to the Bellman equation is given
                    by the following equation
                          $$ Q*(s, a) = \sum_{s'}p(s'|s, a)(R(s, a, s') + \gamma*\max_a Q(s', a)) $$
                    here Q(s, a) is the Q-value of a state action pair, p(s'|s, a) is the probability of reaching state s'
                    by performing action a from state s (by encorporating probability of a transition we are taking into
                    account the uncertainity associated with taking an action),  R(s, a, s') is the reward for the action and \( \gamma \) is the discount factor which
                    represents by how much should future rewards should affect actions in current state. A high \( \gamma \) would
                    lead to the bot favoring higher rewards in the future where as lower values will make it look for more
                    immediate rewards
                  </p>
                  <p>
                    In our current scenario as we are playing a game in we can assume that whatever action we take gets executed
                    with 100% certainity, thus the above equation becomes:
                     $$ Q*(s, a) = R(s, a) + \gamma*\max_a Q(s', a) $$
                    One question that came to my mind while training the neural network was that why to do it this way
                    i.e. why use a neural network which is very very hard to train when you can simply use more traditional
                    methods like value iteration and policy iteration which are deterministic and easy to implement. The reason
                    for this is that though Q value update using Bellman is correct in theory (Bellman equation basically says
                    start optimal and be optimal at every step of the way) its implementation using value or policy iteration
                    uses discrete state spaces, which is something we rarely come accross in real world problems. We could
                    always try and discretize the space but this would usually lead us to store a lot of state information,
                    many of which would be about nearby states which can have similar Q values. Neural network solves this
                    exact problem, we can use it to produce the Q(s, a) values for a continous state space.
                  </p>
                  <p>
                    Now that we know what the Q-value equation looks like and why we should use neural network to solve it, lets
                    look at how exactly we need to do this. Anyone familiar with neural network or with any other optimization
                    strategy in general will know that we need a loss value to minimize in order for things to work. A good loss
                    value in this case can be the mean squared difference between the Q-value returned by the Bellman equation
                    and the one returned by the neural network. This makes sense as we would like to train our model such that
                    the Q-values produced are exactly same as what we would have achieved using methods like value iteration if
                    we would have discretized the state space. So our loss function should looks like the following.
                    $$ L = mean((Bellman - Q(s, a))^2) $$
                    Subsituiting the equation for Bellman value we solved above, we get
                    $$ L = mean((R(s, a) + \gamma*\max_a Q(s', a) - Q(s, a))^2) $$
                  </p>
                  <h4>Network Architecture and Loss Calculation</h4>
                  <p>
                    Now that we have all the maths figured out, its time to get into the technical details and implement it
                    using Tensorflow. First, lets define the our model.
                    <div style="background: #ffffff; overflow:auto;width:auto;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">build_model</span>(observation_input, trainable<span style="color: #333333">=</span><span style="color: #007020">True</span>):
        hidden <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>layers<span style="color: #333333">.</span>dense(observation_input, <span style="color: #0000DD; font-weight: bold">64</span>, activation <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>nn<span style="color: #333333">.</span>relu, trainable <span style="color: #333333">=</span> trainable, name <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;dense&#39;</span>)
        hidden_2 <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>layers<span style="color: #333333">.</span>dense(hidden, <span style="color: #0000DD; font-weight: bold">64</span>, activation <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>nn<span style="color: #333333">.</span>relu, trainable <span style="color: #333333">=</span> trainable, name <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;dense_1&#39;</span>)
        hidden_3 <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>layers<span style="color: #333333">.</span>dense(hidden_2, <span style="color: #0000DD; font-weight: bold">64</span>, activation <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>nn<span style="color: #333333">.</span>relu, trainable <span style="color: #333333">=</span> trainable, name <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;dense_2&#39;</span>)
        action_values <span style="color: #333333">=</span> tf<span style="color: #333333">.</span>squeeze(tf<span style="color: #333333">.</span>layers<span style="color: #333333">.</span>dense(hidden_3, env<span style="color: #333333">.</span>action_space<span style="color: #333333">.</span>n, trainable <span style="color: #333333">=</span> trainable, name <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&quot;qValueLayer&quot;</span>))
        <span style="color: #008800; font-weight: bold">return</span> action_values
</pre></div>

                  </p>
                  <p>
                    The build_model function takes 2 input variables "observation_input" which is a tensorflow place holder for the input data and
                    "trainable" is a boolean variable which determines whether or not a network needs to be trained. This is used to create a target network,
                    more on this later. The output dimension of the network is set to "env.action_space.n" (env is the object reference to the OpenAI's lunar
                    landar gym) which gives the number of actions possible, which in this case is 4. So what we are trying to do with this network model
                    is to get the Q-values associated with all the actions for a given state, as the input to this network will be states spaces and the output will
                    be the Q-value associated with each of the actions. This is exactly how we were supposed to use a neural network i.e. use it to get
                    Q-values for the state action pair.
                  </p>
                  <p>
                    Some subtle points about this network, the weights of the layers are initialized using the xavier or glorot initialization,
                    it is actually the default initialization scheme in Tensorflow. All, the biases
                    are all intialized to 0. The reason I am pointing these out is cause whether or not a network learns anything or someting
                    depends on how the weights and biases were initialized.
                  </p>
                  <p>
                    Now that we have a network set up its time to define the loss. As specified earlier the loss should sort of the difference
                    between the ideal output and the actual output received. Lets first define what the ideal output should have been. If the
                    state is the terminal i.e. the game is over the ideal output is the reward achieved, i.e.
                    $$ Q*(s, a) = R(s, a)$$
                    If the state is not terminal then the ideal Q-value is given by:
                    $$ Q*(s, a) = R(s, a) + \gamma*\max_a Q(s', a) $$
                    A thing to note here is that the ideal output is a 1 D vector, hence the actual output has also got to be a one dimensional vector
                    as well. Remember that the network we defined above gives a 4 D vector as output for each input observation passed. This is something
                    that confused me for quite some time as I couldn't understand how to use the network's out. After a few frustrating hours and
                    going back and forth in defining my loss I understood that what I actually need for the network's output is Q(s, a) that is the
                    value returned by the network for a particular action. Basically for each state we get a 4 D output, from this 4 D vector we
                    need to choose the value corresponding to the action that was actually taken in that state. Doing this for all input observations
                    we get a 1 D, which we can then use for calculating the loss.
                  </p>
                  <p>
                    With the network architecture and the loss function figured out, the only major part that remains to be setup is the eplison
                    greedy strategy for exploration vs exploitation. But before that I feel now is a right time to talk about some techical/engineering
                    things to make the network learn properly.
                    <ol>
                        <li><i>Replay Memory</i>: Use a replay memory which is basically a list to store the observations i.e. (R, S, A, S'). Here
                          R is the reward returned, S is the current state, A the action taken from that state and S' the state where
                          the bot reached after taking te action. At each network update step sample observations of the required batch
                          size from this list. The reason for doing this is that neural networks needs samples which are not correlated and
                          subsequent observations in a game will always be correlated to each other. So to solve this problem of correlation,
                          we can store all the observations seen till now and then randomly sample them.
                        </li>
                        <li><i>Target Network</i>: The way refinforcement learning usually works is that the enviroment provides a feedback
                          about the actions a bot/agent takes and then based on this feedback bot/agent adjusts its decision system. This feedback
                          creates a problem for training a neural network as this feedback prevents the network from generalizing well. To solve,
                          we use what is called a Target network, which we won't be training (this is what the trainable parameter was for in the build_model function)
                          . What we will do for this network is that we will copy the weights from the network we are training to this one at certain
                          intervals (it is a hyperparameter). We will be using this network for calculating the ideal output vector.
                         </li>
                        <li><i>Huber Loss</i>: The loss that I defined earlier was Mean Squared error though this is the loss function the team at
                          Deep Mind used to train its network but I found it very unstable. Instead I used
                          <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber Loss</a> to train my model as it is more robust to outliers and hence
                          more stable.
                        </li>
                    </ol>
                  </p>
                  <h4>Epsilon Greedy Strategy</h4>
                  <p>
                    The final piece of the puzzle to make the agent learn is to decide on the Epsilon Greedy Strategy. It
                    deals with what actions an agent takes given an observation. The way it is defined that the agent
                    performs random action with Epsilon probability and uses the learned knowledge the rest of the time.
                    Pseudo code for Epsilon Greedy Strategy look like the following
                    <div style="background: #ffffff; overflow:auto;width:auto;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">takeAction</span>(Agent, Observation, epsilon):
       <span style="color: #008800; font-weight: bold">return</span> Action<span style="color: #333333">.</span>Random() <span style="color: #008800; font-weight: bold">if</span> random() <span style="color: #333333">&lt;</span> epsilon <span style="color: #008800; font-weight: bold">else</span> Agent<span style="color: #333333">.</span>getAction(Observation)
</pre></div>

                </p>
                <p>
                    What we ideally want to do is explore a lot of states in the beginining by taking random actions
                     and then as the agent learn make the actions less and less random and more based on the information that
                     the agent has learned its environment. The reason for doing this is that in the beginining the agent
                     knows nothing about the environment it is in and if we let it perform actions based on what it knows it won't
                      properly explore all the states and may miss out on a higher reward. We also want the randomness
                      to die down as the agent learns so that it can solidify its understanding of the enviroment by acting on
                      what it knows about it.
                  </p>
                  <p>
                    There can be multiple different strategies that can work, linear decay, exponential decay and so on. What worked
                    for me was to let the epsilon value decay linearly for the first 50000 observations and then decay it exponentially
                    from there on. Following is how my agent worked after 4000 episodes of training, one episode is one complete round of
                    the game.
                  </p>
                  <figure>
                      <img style = "display: block;margin: 0 auto; width:50%" src="../resources/images/lunarLandar.gif" alt='Lunar Lander Game' />
                      <figcaption style = "text-align:center">My Agent after 4000 episodes of training</figcaption>
                  </figure>
                  <p>
                    Lastly here are some general purpose tips for training a neural network.
                    <ol>
                      <li>Big Batch Size: Ensure that the Batch size is big enough as small batch size leads to unstability in the loss.</li>
                      <li>Use TensorBoard to Visualize network: This is very important as this helps in debugging the model</li>
                      <li>Use Small learning rate: Always try and use a small learning rate in the order of 10<sup>-3</sup>.
                        Further using something like Nestrov momentum always help </li>
                      <li>Add Regulaization Term: A possible problem with the network could be that the weights of your layer are blowing up
                      during training in such cases adding a regulaization term for the layer weights to the loss value helps</li>
                      <li>Check network initialization: A possible problem why the network is nit learning anything is cause the weights
                      and biases were not initialized properly</li>
                      <li>Normalize the input values: This is important while training neural networks as the loss is dependent on the distance.
                      So bigger values will have a higher impact on the loss.</li>
                    </ol>
                  If after following the above steps you still cannot figure out what is wrong with your network. I would suggest reading
                  this <a href = "https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607">post on debugging neural networks</a>.
                  My implementation of the Lunar Lander agent can be found <a href = "https://github.com/monkeydunkey/Lunar_Lander_v2">here.</a>
                  </p>
                  <p>
                    Thats all for now. If you have any questions please leave a comment!
                  </p>
                  <p>
                    Cheers!!
                  </p>
                </div><!-- /.blog-post -->
              </div><!-- /.blog-main -->
            </div><!-- /.row -->
            <div id="disqus_thread"></div>
            <script>
                /**
                *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
                /*
                var disqus_config = function () {
                this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
                this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                };
                */
                (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');
                s.src = 'https://https-monkeydunkey-github-io.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div><!-- /.container -->

    <footer class="blog-footer">
      <p>Blog template built for <a href="http://getbootstrap.com">Bootstrap</a> by <a href="https://twitter.com/mdo">@mdo</a>.</p>
      <p>A shout out to Rob Hust for the fantastic icon.</p>
      <p>
        <a href="#">Back to top</a>
      </p>
    </footer>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../ThirdParty/bootstrap/js/bootstrap.min.js"></script>

  </body>
</html>
